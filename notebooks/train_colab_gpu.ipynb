{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9d43eb551d34e6ca0d2f6a2ae245725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f59fb0b7e33463587b052b9427c47a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f49f85d6a1244950b8c95ee6677936f5",
              "IPY_MODEL_6c6438726fc04bffb7d68a93348f486b"
            ]
          }
        },
        "5f59fb0b7e33463587b052b9427c47a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f49f85d6a1244950b8c95ee6677936f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_57ddc78a20e643dda0184a65a0042361",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_942ba1a587474a488a3752182ce31665"
          }
        },
        "6c6438726fc04bffb7d68a93348f486b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e9aa0f696824dfa9f531a83e1949fde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:15&lt;00:00, 29.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a8ec46dbc744570842b1d039043bdfd"
          }
        },
        "57ddc78a20e643dda0184a65a0042361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "942ba1a587474a488a3752182ce31665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e9aa0f696824dfa9f531a83e1949fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a8ec46dbc744570842b1d039043bdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "091b340f6e6a4ce2bbeca43704fa944d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61852593b7794ea997db5f00381bcb29",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1628ee35762549499ebe8fca7b547f00",
              "IPY_MODEL_24c5f4eda2814d57a224573f7cbe990c"
            ]
          }
        },
        "61852593b7794ea997db5f00381bcb29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1628ee35762549499ebe8fca7b547f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf2e6b1664fc4dbbb70bfa7ffd47fbc2",
            "_dom_classes": [],
            "description": "Epoch:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf0d0d20f1ee449592d22f08d52f23cc"
          }
        },
        "24c5f4eda2814d57a224573f7cbe990c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b2c485ed19e4d9b8d8b457cb3b9e29a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_679afbb04ebe4178b7ce54cb23d28ff4"
          }
        },
        "bf2e6b1664fc4dbbb70bfa7ffd47fbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf0d0d20f1ee449592d22f08d52f23cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b2c485ed19e4d9b8d8b457cb3b9e29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "679afbb04ebe4178b7ce54cb23d28ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29c5d1907a8245c3990d8a567f29c2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be0935ce53364f55a12580076601ff1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_786879b25be943d2bda169b3bc5ecbc9",
              "IPY_MODEL_effb8b65364745f79a8a3816a154ab1e"
            ]
          }
        },
        "be0935ce53364f55a12580076601ff1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "786879b25be943d2bda169b3bc5ecbc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a7360efb58ef4603bc487d5e5ad2342c",
            "_dom_classes": [],
            "description": "Iteration:   1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 42365,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 329,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27d9e9c9f4414d2eb4aa4babae3282d0"
          }
        },
        "effb8b65364745f79a8a3816a154ab1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3308b91ae33542c3b7b1b5598d4d699f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 329/42365 [00:27&lt;1:00:43, 11.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95c63806fc7c4750bbc92f1653a333ec"
          }
        },
        "a7360efb58ef4603bc487d5e5ad2342c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27d9e9c9f4414d2eb4aa4babae3282d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3308b91ae33542c3b7b1b5598d4d699f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95c63806fc7c4750bbc92f1653a333ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQbQukSjSry"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEzQvlXvHWTj"
      },
      "source": [
        "prereqs:\n",
        "- download https://github.com/matt-raporte/anli and save it to your drive under ColabNotebooks/anli\n",
        "- supress disconects step #7 https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBo50yaVW7Ww",
        "outputId": "396120b9-1b9a-490d-f83d-f6398f7bb501"
      },
      "source": [
        "# Mount into d\\rive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1YXumGLaY2F",
        "outputId": "5949e21d-71ed-47e2-f574-7971ee09c8f7"
      },
      "source": [
        "%%bash\n",
        "pip install torch==1.7\n",
        "pip install transformers==3.0.2\n",
        "pip install flint\n",
        "pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.7\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (3.7.4.3)\n",
            "Installing collected packages: dataclasses, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting flint\n",
            "  Downloading flint-0.2.tar.gz (6.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from flint) (57.2.0)\n",
            "Collecting pep8>=1.4.3\n",
            "  Downloading pep8-1.7.1-py2.py3-none-any.whl (41 kB)\n",
            "Collecting pyflakes>=0.6.1\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "Building wheels for collected packages: flint\n",
            "  Building wheel for flint (setup.py): started\n",
            "  Building wheel for flint (setup.py): finished with status 'done'\n",
            "  Created wheel for flint: filename=flint-0.2-py3-none-any.whl size=6466 sha256=1456c181b93dfa1bb50471ffc2216a3b3e389d59d6b7dea1ba3bae5f043884bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c1/1f/33c8728944adbac46dcbfb47bc6587fa0b09dd22566061ae57\n",
            "Successfully built flint\n",
            "Installing collected packages: pyflakes, pep8, flint\n",
            "Successfully installed flint-0.2 pep8-1.7.1 pyflakes-2.3.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeWbTJwMfuPH",
        "outputId": "9f5b30bd-6210-4aa4-e085-932a49309937"
      },
      "source": [
        "%%bash\n",
        "cd drive/MyDrive/ColabNotebooks/anli\n",
        "source setup.sh\n",
        "bash script/download_data.sh\n",
        "python src/dataset_tools/build_data.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PYTHONPATH=/env/python:/content/drive/MyDrive/ColabNotebooks/anli/src:/content/drive/MyDrive/ColabNotebooks/anli/utest\n",
            "The path of project root: /content/drive/MyDrive/ColabNotebooks/anli\n",
            "SNLI checked.\n",
            "MNLI checked.\n",
            "FEVER NLI checked.\n",
            "ANLI checked.\n",
            "Data download completed and checked.\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_test.jsonl\n",
            "SNLI examples without gold label have been filtered.\n",
            "SNLI Train size: 549367\n",
            "SNLI Dev size: 9842\n",
            "SNLI Test size: 9824\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_dev_matched.jsonl\n",
            "MNLI examples without gold label have been filtered.\n",
            "MNLI Train size: 392702\n",
            "MNLI MisMatched Dev size: 9832\n",
            "MNLI Matched dev size: 9815\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/mm_dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/m_dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/train_fitems.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/dev_fitems.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/test_fitems.jsonl\n",
            "FEVER-NLI Train size: 208346\n",
            "FEVER-NLI Dev size: 19998\n",
            "FEVER-NLI Test size: 19998\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/test.jsonl\n",
            "ANLI (R1) Train size: 16946\n",
            "ANLI (R1) Dev size: 1000\n",
            "ANLI (R1) Test size: 1000\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/test.jsonl\n",
            "ANLI (R2) Train size: 45460\n",
            "ANLI (R2) Dev size: 1000\n",
            "ANLI (R2) Test size: 1000\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/test.jsonl\n",
            "ANLI (R3) Train size: 100459\n",
            "ANLI (R3) Dev size: 1200\n",
            "ANLI (R3) Test size: 1200\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/test.jsonl\n",
            "NLI data built!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.74it/s]\r107it [00:00,  8.16it/s]\r2876it [00:00, 11.66it/s]\r5874it [00:00, 16.66it/s]\r10759it [00:00, 23.79it/s]\r16307it [00:00, 33.99it/s]\r20773it [00:00, 48.53it/s]\r24437it [00:01, 69.23it/s]\r28000it [00:01, 98.81it/s]\r31331it [00:01, 140.98it/s]\r35975it [00:01, 201.14it/s]\r41997it [00:01, 286.93it/s]\r47018it [00:01, 408.90it/s]\r52020it [00:01, 582.11it/s]\r57159it [00:01, 827.56it/s]\r63011it [00:01, 1175.11it/s]\r68142it [00:02, 1660.88it/s]\r74562it [00:02, 2346.67it/s]\r80961it [00:02, 3300.50it/s]\r86678it [00:02, 4398.66it/s]\r91797it [00:02, 6060.58it/s]\r96547it [00:02, 8172.35it/s]\r103178it [00:02, 11088.96it/s]\r111281it [00:02, 14963.70it/s]\r118844it [00:03, 19705.66it/s]\r126974it [00:03, 25501.81it/s]\r133988it [00:03, 27873.78it/s]\r142522it [00:03, 34929.83it/s]\r150632it [00:03, 42124.11it/s]\r158649it [00:03, 49116.35it/s]\r167068it [00:03, 56131.37it/s]\r174783it [00:03, 55510.52it/s]\r181807it [00:04, 34523.00it/s]\r187274it [00:04, 38292.02it/s]\r192643it [00:04, 41167.54it/s]\r197899it [00:04, 41184.43it/s]\r202815it [00:04, 29859.61it/s]\r206872it [00:04, 32426.42it/s]\r211088it [00:05, 34839.06it/s]\r217237it [00:05, 40045.38it/s]\r221931it [00:05, 25447.21it/s]\r230330it [00:05, 32175.13it/s]\r238815it [00:05, 39538.81it/s]\r247304it [00:05, 47084.75it/s]\r255736it [00:05, 54273.85it/s]\r264305it [00:05, 60980.87it/s]\r272286it [00:06, 65624.38it/s]\r280460it [00:06, 43090.40it/s]\r288853it [00:06, 50396.33it/s]\r297395it [00:06, 57464.75it/s]\r305801it [00:06, 63489.62it/s]\r314333it [00:06, 68767.30it/s]\r322651it [00:06, 72536.26it/s]\r331182it [00:07, 75941.71it/s]\r339367it [00:07, 77216.02it/s]\r348045it [00:07, 79854.11it/s]\r356386it [00:07, 42773.98it/s]\r364785it [00:07, 50157.23it/s]\r373350it [00:07, 57274.62it/s]\r381984it [00:07, 63708.03it/s]\r390409it [00:08, 68734.83it/s]\r398500it [00:08, 71984.15it/s]\r407063it [00:08, 75598.14it/s]\r415396it [00:08, 77761.30it/s]\r423688it [00:08, 78955.95it/s]\r432018it [00:08, 80208.15it/s]\r440554it [00:08, 81685.94it/s]\r449108it [00:09, 38917.33it/s]\r457342it [00:09, 46230.41it/s]\r464798it [00:09, 52177.31it/s]\r472530it [00:09, 57817.05it/s]\r480105it [00:09, 62235.54it/s]\r487643it [00:09, 65670.86it/s]\r495288it [00:09, 68482.22it/s]\r502806it [00:09, 70361.41it/s]\r510294it [00:09, 70127.78it/s]\r517624it [00:10, 70779.77it/s]\r524925it [00:10, 70782.30it/s]\r532897it [00:10, 73244.38it/s]\r540353it [00:10, 73557.44it/s]\r547801it [00:10, 68846.01it/s]\r550152it [00:10, 52390.86it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.89it/s]\r1048it [00:00,  6.99it/s]\r2410it [00:00,  9.98it/s]\r8451it [00:00, 14.26it/s]\r10000it [00:00, 15847.55it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.79it/s]\r1597it [00:00,  8.27it/s]\r3603it [00:00, 11.82it/s]\r6823it [00:01, 16.87it/s]\r10000it [00:01, 9231.40it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.04it/s]\r3711it [00:00,  8.63it/s]\r7702it [00:00, 12.33it/s]\r10043it [00:00, 17.61it/s]\r12178it [00:00, 25.15it/s]\r14795it [00:00, 35.91it/s]\r18184it [00:00, 51.28it/s]\r21393it [00:00, 73.21it/s]\r24171it [00:00, 104.47it/s]\r26916it [00:01, 148.90it/s]\r29433it [00:01, 211.90it/s]\r32345it [00:01, 301.78it/s]\r35540it [00:01, 429.37it/s]\r38218it [00:01, 601.24it/s]\r40642it [00:01, 849.88it/s]\r43673it [00:01, 1199.68it/s]\r46143it [00:02, 1668.02it/s]\r51480it [00:02, 2351.38it/s]\r57459it [00:02, 3303.43it/s]\r63687it [00:02, 4614.28it/s]\r70018it [00:02, 6391.99it/s]\r76583it [00:02, 8765.57it/s]\r83222it [00:02, 11851.60it/s]\r89902it [00:02, 15734.40it/s]\r96516it [00:02, 20398.00it/s]\r102799it [00:02, 25559.41it/s]\r109159it [00:03, 31148.23it/s]\r115823it [00:03, 37070.63it/s]\r122311it [00:03, 42539.88it/s]\r128829it [00:03, 47488.04it/s]\r135612it [00:03, 52181.91it/s]\r142167it [00:03, 42095.56it/s]\r148809it [00:03, 47290.29it/s]\r155005it [00:03, 50905.14it/s]\r160934it [00:04, 42759.32it/s]\r166018it [00:04, 38831.91it/s]\r170536it [00:04, 33739.68it/s]\r174474it [00:04, 23729.68it/s]\r177651it [00:04, 21881.97it/s]\r180431it [00:05, 21791.90it/s]\r183025it [00:05, 21993.06it/s]\r185515it [00:05, 22241.60it/s]\r187943it [00:05, 22612.77it/s]\r190349it [00:05, 13727.81it/s]\r194475it [00:05, 17163.29it/s]\r198958it [00:05, 21062.37it/s]\r203426it [00:05, 25031.36it/s]\r207781it [00:06, 28691.09it/s]\r212002it [00:06, 31740.34it/s]\r216073it [00:06, 33985.26it/s]\r220791it [00:06, 37097.35it/s]\r226845it [00:06, 41971.97it/s]\r231593it [00:06, 42310.62it/s]\r237614it [00:06, 46453.55it/s]\r242653it [00:07, 28371.71it/s]\r248833it [00:07, 33867.40it/s]\r255175it [00:07, 39370.31it/s]\r261039it [00:07, 43675.31it/s]\r266789it [00:07, 47069.90it/s]\r273214it [00:07, 51173.70it/s]\r279696it [00:07, 54622.60it/s]\r285744it [00:07, 56255.34it/s]\r291791it [00:07, 57455.46it/s]\r298069it [00:07, 58953.27it/s]\r304162it [00:08, 32553.89it/s]\r310515it [00:08, 38130.83it/s]\r316666it [00:08, 43037.65it/s]\r323114it [00:08, 47805.67it/s]\r329570it [00:08, 51840.35it/s]\r335729it [00:08, 54424.07it/s]\r342213it [00:08, 57174.83it/s]\r348486it [00:09, 58732.24it/s]\r354688it [00:09, 58130.55it/s]\r361072it [00:09, 59571.67it/s]\r367247it [00:09, 60206.65it/s]\r373637it [00:09, 61267.75it/s]\r379972it [00:09, 61877.28it/s]\r386224it [00:09, 30430.30it/s]\r392078it [00:10, 35551.55it/s]\r392702it [00:10, 38945.73it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.94it/s]\r1403it [00:00,  7.05it/s]\r3012it [00:00, 10.07it/s]\r4106it [00:00, 14.39it/s]\r8558it [00:00, 20.55it/s]\r10000it [00:00, 15465.94it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.12it/s]\r2023it [00:00,  7.32it/s]\r2630it [00:00, 10.45it/s]\r3633it [00:00, 14.92it/s]\r8635it [00:00, 21.31it/s]\r10000it [00:00, 15405.08it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.42it/s]\r4545it [00:00,  7.74it/s]\r8320it [00:00, 11.05it/s]\r12177it [00:00, 15.79it/s]\r14693it [00:00, 22.53it/s]\r16653it [00:01, 32.04it/s]\r27545it [00:01, 45.77it/s]\r38548it [00:02, 65.37it/s]\r49581it [00:02, 93.36it/s]\r60645it [00:02, 133.32it/s]\r71940it [00:02, 190.36it/s]\r82933it [00:02, 271.75it/s]\r93779it [00:02, 387.79it/s]\r104885it [00:02, 553.16it/s]\r115463it [00:02, 788.47it/s]\r126641it [00:02, 1122.98it/s]\r137893it [00:02, 1597.43it/s]\r149090it [00:03, 2268.18it/s]\r160508it [00:03, 3212.90it/s]\r171613it [00:03, 4533.41it/s]\r182845it [00:03, 6366.17it/s]\r193978it [00:03, 8876.24it/s]\r205321it [00:03, 12268.87it/s]\r208346it [00:03, 58678.11it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.96it/s]\r5431it [00:00,  8.52it/s]\r12073it [00:00, 12.17it/s]\r18061it [00:00, 17.38it/s]\r19998it [00:00, 39500.61it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.18it/s]\r6828it [00:00,  8.83it/s]\r8877it [00:00, 12.61it/s]\r11384it [00:00, 18.01it/s]\r19998it [00:00, 36495.08it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.99it/s]\r5139it [00:00,  8.56it/s]\r8576it [00:00, 12.23it/s]\r16946it [00:00, 30335.47it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.60it/s]\r347it [00:00,  9.42it/s]\r1000it [00:00, 3190.63it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.09it/s]\r349it [00:00,  8.67it/s]\r1000it [00:00, 2213.91it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.34it/s]\r2388it [00:00,  7.63it/s]\r7824it [00:00, 10.90it/s]\r17752it [00:00, 15.57it/s]\r22223it [00:00, 22.24it/s]\r32127it [00:00, 31.77it/s]\r41774it [00:00, 45.38it/s]\r45460it [00:00, 51921.83it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.30it/s]\r361it [00:00,  9.00it/s]\r1000it [00:00, 2642.67it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.40it/s]\r349it [00:00,  7.71it/s]\r1000it [00:00, 2837.76it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.78it/s]\r4195it [00:00,  8.26it/s]\r5882it [00:00, 11.79it/s]\r6968it [00:00, 16.82it/s]\r7837it [00:01, 23.98it/s]\r18287it [00:01, 34.25it/s]\r28519it [00:01, 48.92it/s]\r38831it [00:01, 69.87it/s]\r48747it [00:01, 99.79it/s]\r58979it [00:01, 142.50it/s]\r69511it [00:01, 203.45it/s]\r79971it [00:01, 290.40it/s]\r90257it [00:01, 414.36it/s]\r100459it [00:01, 50580.86it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.18it/s]\r343it [00:00,  8.82it/s]\r1200it [00:00, 2975.87it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.12it/s]\r349it [00:00,  5.88it/s]\r1200it [00:00, 2363.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsdAUPE0O0Iq"
      },
      "source": [
        "#flint.data_utils.fields -> import wasn't working for some reason\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under Creative Commons-Non Commercial 4.0 found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class FlintField(object):\n",
        "    @classmethod\n",
        "    def batching(cls, batched_data):\n",
        "        raise NotImplemented()\n",
        "\n",
        "\n",
        "class RawFlintField(FlintField):\n",
        "    @classmethod\n",
        "    def batching(cls, batched_data):\n",
        "        return batched_data\n",
        "\n",
        "\n",
        "class LabelFlintField(FlintField):\n",
        "    def batching(self, batched_data):\n",
        "        return torch.tensor(batched_data)\n",
        "\n",
        "\n",
        "class ArrayIndexFlintField(FlintField):\n",
        "    def __init__(self, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False) -> None:\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.left_pad = left_pad\n",
        "        self.move_eos_to_beginning = move_eos_to_beginning\n",
        "\n",
        "    def collate_tokens(self, values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n",
        "        \"\"\"\n",
        "        Convert a list of 1d tensors into a padded 2d tensor.\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(values[0]):\n",
        "            values = [torch.tensor(v) for v in values]\n",
        "\n",
        "        size = max(v.size(0) for v in values)\n",
        "        res = values[0].new(len(values), size).fill_(pad_idx)\n",
        "\n",
        "        def copy_tensor(src, dst):\n",
        "            assert dst.numel() == src.numel()\n",
        "            if move_eos_to_beginning:\n",
        "                assert src[-1] == eos_idx\n",
        "                dst[0] = eos_idx\n",
        "                dst[1:] = src[:-1]\n",
        "            else:\n",
        "                dst.copy_(src)\n",
        "\n",
        "        for i, v in enumerate(values):\n",
        "            copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
        "        return res\n",
        "\n",
        "    def batching(self, batched_data):\n",
        "        return self.collate_tokens(batched_data,\n",
        "                                   self.pad_idx,\n",
        "                                   self.eos_idx,\n",
        "                                   self.left_pad,\n",
        "                                   self.move_eos_to_beginning)\n",
        "\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under Creative Commons-Non Commercial 4.0 found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "from typing import Dict, Type\n",
        "\n",
        "\n",
        "class BaseBatchBuilder(object):\n",
        "    def __init__(self, batching_schema: Dict[str, FlintField]) -> None:\n",
        "        super().__init__()\n",
        "        self.batching_schema: Dict[str, FlintField] = batching_schema\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        field_names = batch[0].keys()\n",
        "        batched_data = dict()\n",
        "\n",
        "        for field_name in field_names:\n",
        "            if field_name not in self.batching_schema:\n",
        "                # default is RawFlintField\n",
        "                batched_data[field_name] = RawFlintField.batching([item[field_name] for item in batch])\n",
        "\n",
        "            else:\n",
        "                batched_data[field_name] = self.batching_schema[field_name].batching([item[field_name] for item in batch])\n",
        "\n",
        "        return batched_data\n",
        "\n",
        "\n",
        "def has_tensor(obj) -> bool:\n",
        "    \"\"\"\n",
        "    Given a possibly complex data structure,\n",
        "    check if it has any torch.Tensors in it.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return True\n",
        "    elif isinstance(obj, dict):\n",
        "        return any(has_tensor(value) for value in obj.values())\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return any(has_tensor(item) for item in obj)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def move_to_device(obj, cuda_device: int):\n",
        "    \"\"\"\n",
        "    Given a structure (possibly) containing Tensors on the CPU,\n",
        "    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).\n",
        "    \"\"\"\n",
        "\n",
        "    if cuda_device < 0 or not has_tensor(obj):\n",
        "        return obj\n",
        "    elif isinstance(obj, torch.Tensor):\n",
        "        return obj.cuda(cuda_device)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: move_to_device(value, cuda_device) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [move_to_device(item, cuda_device) for item in obj]\n",
        "    elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n",
        "        # This is the best way to detect a NamedTuple, it turns out.\n",
        "        return obj.__class__(*(move_to_device(item, cuda_device) for item in obj))\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(move_to_device(item, cuda_device) for item in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBmc2KLT_BI"
      },
      "source": [
        "## imports and helpers for training\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler, RandomSampler, SequentialSampler\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm, trange\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"drive/MyDrive/ColabNotebooks/anli/src\")\n",
        "\n",
        "import config\n",
        "from utils import common, list_dict_data_tool, save_tool\n",
        "#from flint.data_utils.fields import RawFlintField, LabelFlintField, ArrayIndexFlintField\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert-base\": {\n",
        "        \"model_name\": \"bert-base-uncased\",\n",
        "        \"tokenizer\": BertTokenizer,\n",
        "        \"sequence_classification\": BertForSequenceClassification,\n",
        "        # \"padding_token_value\": 0,\n",
        "        \"padding_segement_value\": 0,\n",
        "        \"padding_att_value\": 0,\n",
        "        \"do_lower_case\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "registered_path = {\n",
        "    'snli_train': config.PRO_ROOT / \"data/build/snli/train.jsonl\",\n",
        "    'snli_dev': config.PRO_ROOT / \"data/build/snli/dev.jsonl\",\n",
        "    'snli_test': config.PRO_ROOT / \"data/build/snli/test.jsonl\",\n",
        "\n",
        "    'mnli_train': config.PRO_ROOT / \"data/build/mnli/train.jsonl\",\n",
        "    'mnli_m_dev': config.PRO_ROOT / \"data/build/mnli/m_dev.jsonl\",\n",
        "    'mnli_mm_dev': config.PRO_ROOT / \"data/build/mnli/mm_dev.jsonl\",\n",
        "\n",
        "    'fever_train': config.PRO_ROOT / \"data/build/fever_nli/train.jsonl\",\n",
        "    'fever_dev': config.PRO_ROOT / \"data/build/fever_nli/dev.jsonl\",\n",
        "    'fever_test': config.PRO_ROOT / \"data/build/fever_nli/test.jsonl\",\n",
        "\n",
        "    'anli_r1_train': config.PRO_ROOT / \"data/build/anli/r1/train.jsonl\",\n",
        "    'anli_r1_dev': config.PRO_ROOT / \"data/build/anli/r1/dev.jsonl\",\n",
        "    'anli_r1_test': config.PRO_ROOT / \"data/build/anli/r1/test.jsonl\",\n",
        "\n",
        "    'anli_r2_train': config.PRO_ROOT / \"data/build/anli/r2/train.jsonl\",\n",
        "    'anli_r2_dev': config.PRO_ROOT / \"data/build/anli/r2/dev.jsonl\",\n",
        "    'anli_r2_test': config.PRO_ROOT / \"data/build/anli/r2/test.jsonl\",\n",
        "\n",
        "    'anli_r3_train': config.PRO_ROOT / \"data/build/anli/r3/train.jsonl\",\n",
        "    'anli_r3_dev': config.PRO_ROOT / \"data/build/anli/r3/dev.jsonl\",\n",
        "    'anli_r3_test': config.PRO_ROOT / \"data/build/anli/r3/test.jsonl\",\n",
        "}\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def sample_data_list(d_list, ratio):\n",
        "    if ratio <= 0:\n",
        "        raise ValueError(\"Invalid training weight ratio. Please change --train_weights.\")\n",
        "    upper_int = int(math.ceil(ratio))\n",
        "    if upper_int == 1:\n",
        "        return d_list # if ratio is 1 then we just return the data list\n",
        "    else:\n",
        "        sampled_d_list = []\n",
        "        for _ in range(upper_int):\n",
        "            sampled_d_list.extend(copy.deepcopy(d_list))\n",
        "        if np.isclose(ratio, upper_int):\n",
        "            return sampled_d_list\n",
        "        else:\n",
        "            sampled_length = int(ratio * len(d_list))\n",
        "            random.shuffle(sampled_d_list)\n",
        "            return sampled_d_list[:sampled_length]\n",
        "\n",
        "class NLITransform(object):\n",
        "    def __init__(self, model_name, tokenizer, max_length=None):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        processed_sample = dict()\n",
        "        processed_sample['uid'] = sample['uid']\n",
        "        processed_sample['gold_label'] = sample['label']\n",
        "        processed_sample['y'] = nli_label2index[sample['label']]\n",
        "\n",
        "        # premise: str = sample['premise']\n",
        "        premise: str = sample['context'] if 'context' in sample else sample['premise']\n",
        "        hypothesis: str = sample['hypothesis']\n",
        "\n",
        "        if premise.strip() == '':\n",
        "            premise = 'empty'\n",
        "\n",
        "        if hypothesis.strip() == '':\n",
        "            hypothesis = 'empty'\n",
        "\n",
        "        tokenized_input_seq_pair = self.tokenizer.encode_plus(premise, hypothesis,\n",
        "                                                              max_length=self.max_length,\n",
        "                                                              return_token_type_ids=True, truncation=True)\n",
        "\n",
        "        processed_sample.update(tokenized_input_seq_pair)\n",
        "\n",
        "        return processed_sample\n",
        "\n",
        "def build_eval_dataset_loader_and_sampler(d_list, data_transformer, batching_schema, batch_size_per_gpu_eval):\n",
        "    d_dataset = NLIDataset(d_list, data_transformer)\n",
        "    d_sampler = SequentialSampler(d_dataset)\n",
        "    d_dataloader = DataLoader(dataset=d_dataset,\n",
        "                              batch_size=batch_size_per_gpu_eval,\n",
        "                              shuffle=False,  #\n",
        "                              num_workers=0,\n",
        "                              pin_memory=True,\n",
        "                              sampler=d_sampler,\n",
        "                              collate_fn=BaseBatchBuilder(batching_schema))  #\n",
        "    return d_dataset, d_sampler, d_dataloader\n",
        "\n",
        "class NLIDataset(Dataset):\n",
        "    def __init__(self, data_list, transform) -> None:\n",
        "        super().__init__()\n",
        "        self.d_list = data_list\n",
        "        self.len = len(self.d_list)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        return self.transform(self.d_list[index])\n",
        "\n",
        "    # you should write schema for each of the input elements\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "nli_label2index = {\n",
        "    'e': 0,\n",
        "    'n': 1,\n",
        "    'c': 2,\n",
        "    'h': -1,\n",
        "}\n",
        "\n",
        "def evaluation_dataset(args, eval_dataloader, eval_list, model, r_dict, eval_name):\n",
        "    # r_dict = dict()\n",
        "    pred_output_list = eval_model(model, eval_dataloader, args.get('global_rank', 0), args)\n",
        "    predictions = pred_output_list\n",
        "    hit, total = count_acc(eval_list, pred_output_list)\n",
        "\n",
        "    print(debug_node_info(args), f\"{eval_name} Acc:\", hit, total, hit / total)\n",
        "\n",
        "    r_dict[f'{eval_name}'] = {\n",
        "        'acc': hit / total,\n",
        "        'correct_count': hit,\n",
        "        'total_count': total,\n",
        "        'predictions': predictions,\n",
        "    }\n",
        "\n",
        "\n",
        "def eval_model(model, dev_dataloader, device_num, args):\n",
        "    model.eval()\n",
        "\n",
        "    uid_list = []\n",
        "    y_list = []\n",
        "    pred_list = []\n",
        "    logits_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dev_dataloader, 0):\n",
        "            batch = move_to_device(batch, device_num)\n",
        "\n",
        "            if args['model_class_name'] in [\"distilbert\", \"bart-large\"]:\n",
        "                outputs = model(batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                labels=batch['y'])\n",
        "            else:\n",
        "                outputs = model(batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                token_type_ids=batch['token_type_ids'],\n",
        "                                labels=batch['y'])\n",
        "\n",
        "            loss, logits = outputs[:2]\n",
        "\n",
        "            uid_list.extend(list(batch['uid']))\n",
        "            y_list.extend(batch['y'].tolist())\n",
        "            pred_list.extend(torch.max(logits, 1)[1].view(logits.size(0)).tolist())\n",
        "            logits_list.extend(logits.tolist())\n",
        "\n",
        "    assert len(pred_list) == len(logits_list)\n",
        "    assert len(pred_list) == len(logits_list)\n",
        "\n",
        "    result_items_list = []\n",
        "    for i in range(len(uid_list)):\n",
        "        r_item = dict()\n",
        "        r_item['uid'] = uid_list[i]\n",
        "        r_item['logits'] = logits_list[i]\n",
        "        r_item['predicted_label'] = id2label[pred_list[i]]\n",
        "\n",
        "        result_items_list.append(r_item)\n",
        "\n",
        "    return result_items_list\n",
        "\n",
        "id2label = {\n",
        "    0: 'e',\n",
        "    1: 'n',\n",
        "    2: 'c',\n",
        "    -1: '-',\n",
        "}\n",
        "\n",
        "def count_acc(gt_list, pred_list):\n",
        "    assert len(gt_list) == len(pred_list)\n",
        "    gt_dict = list_dict_data_tool.list_to_dict(gt_list, 'uid')\n",
        "    pred_list = list_dict_data_tool.list_to_dict(pred_list, 'uid')\n",
        "    total_count = 0\n",
        "    hit = 0\n",
        "    for key, value in pred_list.items():\n",
        "        if gt_dict[key]['label'] == value['predicted_label']:\n",
        "            hit += 1\n",
        "        total_count += 1\n",
        "    return hit, total_count\n",
        "\n",
        "def debug_node_info(args):\n",
        "    names = ['global_rank', 'local_rank', 'node_rank']\n",
        "    values = []\n",
        "\n",
        "    for name in names:\n",
        "        if name in args:\n",
        "            values.append(args[name])\n",
        "        else:\n",
        "            return \"Pro:No node info \"\n",
        "\n",
        "    return \"Pro:\" + '|'.join([f\"{name}:{value}\" for name, value in zip(names, values)]) + \"||Print:\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919,
          "referenced_widgets": [
            "f9d43eb551d34e6ca0d2f6a2ae245725",
            "5f59fb0b7e33463587b052b9427c47a4",
            "f49f85d6a1244950b8c95ee6677936f5",
            "6c6438726fc04bffb7d68a93348f486b",
            "57ddc78a20e643dda0184a65a0042361",
            "942ba1a587474a488a3752182ce31665",
            "0e9aa0f696824dfa9f531a83e1949fde",
            "1a8ec46dbc744570842b1d039043bdfd"
          ]
        },
        "id": "7tkk4T6TIMIb",
        "outputId": "13580ad9-025d-4d56-e29e-696995e78fa6"
      },
      "source": [
        "## build train set & set model args\n",
        "\n",
        "# Args:\n",
        "local_rank = 0 # this is the gpu rank, we only have 1\n",
        "model_class_name='bert-base' # changed from roberta-base for faster training time https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8\n",
        "\n",
        "#eval_data='anli_r1_dev:none,anli_r2_dev:none,anli_r3_dev:none'\n",
        "eval_data='anli_r1_dev:none'\n",
        "#train_data='anli_r1_train:none,anli_r2_train:none,anli_r3_train:none'\n",
        "train_data='anli_r1_train:none'\n",
        "#train_weights='10,20,10'\n",
        "\n",
        "adam_epsilon=1e-08\n",
        "cpu=False\n",
        "debug_mode=False\n",
        "epochs=2\n",
        "eval_frequency=2000\n",
        "experiment_name='roberta-base|snli+mnli+fnli+r1*10+r2*20+r3*10|nli'\n",
        "fp16=False\n",
        "fp16_opt_level='O1'\n",
        "global_iteration=0\n",
        "gpus_per_node=1\n",
        "gradient_accumulation_steps=4\n",
        "learning_rate=1e-05\n",
        "max_grad_norm=1.0\n",
        "max_length=156\n",
        "node_rank=0\n",
        "num_nodes=1\n",
        "per_gpu_eval_batch_size=16\n",
        "per_gpu_train_batch_size=4\n",
        "resume_path=None\n",
        "sampler_seed=-1\n",
        "save_prediction=True\n",
        "seed=1\n",
        "single_gpu=True\n",
        "total_step=-1\n",
        "train_weights='10'\n",
        "warmup_steps=-1\n",
        "weight_decay=0.0\n",
        "world_size=1\n",
        "global_rank = node_rank * gpus_per_node + local_rank\n",
        "args_dict = {'local_rank':local_rank,'model_class_name':model_class_name,'adam_epsilon':adam_epsilon,'cpu':cpu,'debug_mode':debug_mode,'epochs':epochs,'eval_data':eval_data,'eval_frequency':eval_frequency,'experiment_name':experiment_name,'fp16':fp16,'fp16_opt_level':fp16_opt_level,'global_iteration':global_iteration,'gpus_per_node':gpus_per_node,'gradient_accumulation_steps':gradient_accumulation_steps,'learning_rate':learning_rate,'max_grad_norm':max_grad_norm,'max_length':max_length,'node_rank':node_rank,'num_nodes':num_nodes,'per_gpu_eval_batch_size':per_gpu_eval_batch_size,'per_gpu_train_batch_size':per_gpu_train_batch_size,'resume_path':resume_path,'sampler_seed':sampler_seed,'save_prediction:':save_prediction,'seed':seed,'single_gpu':single_gpu,'total_step':total_step,'train_data':train_data,'train_weights':train_weights,'warmup_steps':warmup_steps,'weight_decay':weight_decay,'world_size': world_size, 'global_rank':global_rank, 'local_rank':local_rank }\n",
        "\n",
        "\n",
        "\n",
        "# warmup_steps = 20\n",
        "debug_count = 1000\n",
        "\n",
        "if total_step > 0:\n",
        "  num_epoch = 10000  # if we set total step, num_epoch will be forever.\n",
        "else:\n",
        "  num_epoch = epochs\n",
        "\n",
        "actual_train_batch_size = world_size * per_gpu_train_batch_size * gradient_accumulation_steps\n",
        "actual_train_batch_size = actual_train_batch_size\n",
        "\n",
        "set_seed(seed)\n",
        "num_labels = 3      # we are doing NLI so we set num_labels = 3, for other task we can change this value.\n",
        "\n",
        "max_length = max_length\n",
        "\n",
        "model_class_item = MODEL_CLASSES[model_class_name]\n",
        "model_name = model_class_item['model_name']\n",
        "do_lower_case = model_class_item['do_lower_case'] if 'do_lower_case' in model_class_item else False\n",
        "\n",
        "tokenizer = model_class_item['tokenizer'].from_pretrained(model_name,\n",
        "                                                        cache_dir=str(config.PRO_ROOT / \"trans_cache\"),\n",
        "                                                        do_lower_case=do_lower_case)\n",
        "\n",
        "model = model_class_item['sequence_classification'].from_pretrained(model_name,\n",
        "                                                                  cache_dir=str(config.PRO_ROOT / \"trans_cache\"),\n",
        "                                                                  num_labels=num_labels)\n",
        "\n",
        "padding_token_value = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
        "padding_segement_value = model_class_item[\"padding_segement_value\"]\n",
        "padding_att_value = model_class_item[\"padding_att_value\"]\n",
        "left_pad = model_class_item['left_pad'] if 'left_pad' in model_class_item else False\n",
        "\n",
        "batch_size_per_gpu_train = per_gpu_train_batch_size\n",
        "batch_size_per_gpu_eval = per_gpu_eval_batch_size\n",
        "\n",
        "if not cpu and not single_gpu:\n",
        "  dist.init_process_group(\n",
        "      backend='nccl',\n",
        "      init_method='env://',\n",
        "      world_size=world_size,\n",
        "      rank=global_rank\n",
        "  )\n",
        "\n",
        "train_data_str = train_data\n",
        "train_data_weights_str = train_weights\n",
        "eval_data_str = eval_data\n",
        "\n",
        "train_data_name = []\n",
        "train_data_path = []\n",
        "train_data_list = []\n",
        "train_data_weights = []\n",
        "\n",
        "eval_data_name = []\n",
        "eval_data_path = []\n",
        "eval_data_list = []\n",
        "\n",
        "train_data_named_path = train_data_str.split(',')\n",
        "weights_str = train_data_weights_str.split(',') if train_data_weights_str is not None else None\n",
        "\n",
        "eval_data_named_path = eval_data_str.split(',')\n",
        "\n",
        "for named_path in train_data_named_path:\n",
        "  ind = named_path.find(':')\n",
        "  name = named_path[:ind]\n",
        "  path = name[ind + 1:]\n",
        "  if name in registered_path: # breaks here\n",
        "      d_list = common.load_jsonl(registered_path[name])\n",
        "  else:\n",
        "      d_list = common.load_jsonl(path)\n",
        "\n",
        "  train_data_name.append(name)\n",
        "  train_data_path.append(path)\n",
        "\n",
        "  train_data_list.append(d_list)\n",
        "\n",
        "if weights_str is not None:\n",
        "  for weights in weights_str:\n",
        "      train_data_weights.append(float(weights))\n",
        "else:\n",
        "  for i in range(len(train_data_list)):\n",
        "      train_data_weights.append(1)\n",
        "\n",
        "for named_path in eval_data_named_path:\n",
        "  ind = named_path.find(':')\n",
        "  name = named_path[:ind]\n",
        "  path = name[ind + 1:]\n",
        "  if name in registered_path:\n",
        "      d_list = common.load_jsonl(registered_path[name])\n",
        "  else:\n",
        "      d_list = common.load_jsonl(path)\n",
        "  eval_data_name.append(name)\n",
        "  eval_data_path.append(path)\n",
        "\n",
        "  eval_data_list.append(d_list)\n",
        "\n",
        "assert len(train_data_weights) == len(train_data_list)\n",
        "\n",
        "batching_schema = {\n",
        "  'uid': RawFlintField(),\n",
        "  'y': LabelFlintField(),\n",
        "  'input_ids': ArrayIndexFlintField(pad_idx=padding_token_value, left_pad=left_pad),\n",
        "  'token_type_ids': ArrayIndexFlintField(pad_idx=padding_segement_value, left_pad=left_pad),\n",
        "  'attention_mask': ArrayIndexFlintField(pad_idx=padding_att_value, left_pad=left_pad),\n",
        "}\n",
        "\n",
        "data_transformer = NLITransform(model_name, tokenizer, max_length)\n",
        "# data_transformer = NLITransform(model_name, tokenizer, max_length, with_element=True)\n",
        "\n",
        "eval_data_loaders = []\n",
        "for eval_d_list in eval_data_list:\n",
        "  d_dataset, d_sampler, d_dataloader = build_eval_dataset_loader_and_sampler(eval_d_list, data_transformer,\n",
        "                                                                              batching_schema,\n",
        "                                                                              batch_size_per_gpu_eval)\n",
        "  eval_data_loaders.append(d_dataloader)\n",
        "\n",
        "# Estimate the training size:\n",
        "training_list = []\n",
        "for i in range(len(train_data_list)):\n",
        "  print(\"Build Training Data ...\")\n",
        "  train_d_list = train_data_list[i]\n",
        "  train_d_name = train_data_name[i]\n",
        "  train_d_weight = train_data_weights[i]\n",
        "  cur_train_list = sample_data_list(train_d_list, train_d_weight)  # change later  # we can apply different sample strategy here.\n",
        "  print(f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"\n",
        "        f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\")\n",
        "  training_list.extend(cur_train_list)\n",
        "estimated_training_size = len(training_list)\n",
        "print(\"Estimated training size:\", estimated_training_size)\n",
        "# Estimate the training size ends:\n",
        "\n",
        "# t_total = estimated_training_size // gradient_accumulation_steps * num_epoch\n",
        "# t_total = estimated_training_size * num_epoch // actual_train_batch_size\n",
        "if total_step <= 0:\n",
        "  t_total = estimated_training_size * num_epoch // actual_train_batch_size\n",
        "else:\n",
        "  t_total = total_step\n",
        "\n",
        "if warmup_steps <= 0:  # set the warmup steps to 0.1 * total step if the given warmup step is -1.\n",
        "  warmup_steps = int(t_total * 0.1)\n",
        "\n",
        "if not cpu:\n",
        "  torch.cuda.set_device(local_rank)\n",
        "  model.cuda(local_rank)\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "  {\n",
        "      \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      \"weight_decay\": weight_decay,\n",
        "  },\n",
        "  {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "if resume_path:\n",
        "  print(\"Resume Training\")\n",
        "  global_step = global_iteration\n",
        "  print(\"Resume Global Step: \", global_step)\n",
        "  model.load_state_dict(torch.load(str(Path(resume_path) / \"model.pt\"), map_location=torch.device('cpu')))\n",
        "  optimizer.load_state_dict(torch.load(str(Path(resume_path) / \"optimizer.pt\"), map_location=torch.device('cpu')))\n",
        "  scheduler.load_state_dict(torch.load(str(Path(resume_path) / \"scheduler.pt\"), map_location=torch.device('cpu')))\n",
        "  print(\"State Resumed\")\n",
        "\n",
        "if fp16:\n",
        "  try:\n",
        "      from apex import amp\n",
        "  except ImportError:\n",
        "      raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "  model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
        "\n",
        "if not cpu and not single_gpu:\n",
        "  model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n",
        "                                              output_device=local_rank, find_unused_parameters=True)\n",
        "\n",
        "#args_dict = dict(vars(args))\n",
        "file_path_prefix = '.'\n",
        "if global_rank in [-1, 0]:\n",
        "  print(\"Total Steps:\", t_total)\n",
        "  total_step = t_total\n",
        "  print(\"Warmup Steps:\", warmup_steps)\n",
        "  print(\"Actual Training Batch Size:\", actual_train_batch_size)\n",
        "  print(\"Arguments\", pp.pprint(args_dict))\n",
        "\n",
        "is_finished = False\n",
        "\n",
        "# Let build the logger and log everything before the start of the first training epoch.\n",
        "if global_rank in [-1, 0]:  # only do logging if we use cpu or global_rank=0\n",
        "  resume_prefix = \"\"\n",
        "  # if resume_path:\n",
        "  #     resume_prefix = \"resumed_\"\n",
        "\n",
        "  if not debug_mode:\n",
        "      file_path_prefix, date = save_tool.gen_file_prefix(f\"{experiment_name}\")\n",
        "      # # # Create Log File\n",
        "      # Save the source code.\n",
        "      script_name = \"colab_training\" #os.path.basename(__file__)\n",
        "      with open(os.path.join(file_path_prefix, script_name), 'w') as out_f, open(\"drive/MyDrive/ColabNotebooks/anli/colab_train_log.txt\", 'r') as it:\n",
        "          out_f.write(it.read())\n",
        "          out_f.flush()\n",
        "\n",
        "      # Save option file\n",
        "      common.save_json(args_dict, os.path.join(file_path_prefix, \"json\"))\n",
        "      checkpoints_path = Path(file_path_prefix) / \"checkpoints\"\n",
        "      if not checkpoints_path.exists():\n",
        "          checkpoints_path.mkdir()\n",
        "      prediction_path = Path(file_path_prefix) / \"predictions\"\n",
        "      if not prediction_path.exists():\n",
        "          prediction_path.mkdir()\n",
        "\n",
        "      # if this is a resumed, then we save the resumed path.\n",
        "      if resume_path:\n",
        "          with open(os.path.join(file_path_prefix, \"resume_log.txt\"), 'w') as out_f:\n",
        "              out_f.write(str(resume_path))\n",
        "              out_f.flush()\n",
        "\n",
        "# print(f\"Global Rank:{global_rank} ### \", 'Init!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d43eb551d34e6ca0d2f6a2ae245725",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2760it [00:00, 25254.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/train.jsonl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16946it [00:00, 43591.39it/s]\n",
            "1000it [00:00, 74122.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/dev.jsonl\n",
            "Build Training Data ...\n",
            "Data Name:anli_r1_train; Weight: 10.0; Original Size: 16946; Sampled Size: 169460\n",
            "Estimated training size: 169460\n",
            "Total Steps: 21182\n",
            "Warmup Steps: 2118\n",
            "Actual Training Batch Size: 16\n",
            "{ 'adam_epsilon': 1e-08,\n",
            "  'cpu': False,\n",
            "  'debug_mode': False,\n",
            "  'epochs': 2,\n",
            "  'eval_data': 'anli_r1_dev:none',\n",
            "  'eval_frequency': 2000,\n",
            "  'experiment_name': 'roberta-base|snli+mnli+fnli+r1*10+r2*20+r3*10|nli',\n",
            "  'fp16': False,\n",
            "  'fp16_opt_level': 'O1',\n",
            "  'global_iteration': 0,\n",
            "  'global_rank': 0,\n",
            "  'gpus_per_node': 1,\n",
            "  'gradient_accumulation_steps': 4,\n",
            "  'learning_rate': 1e-05,\n",
            "  'local_rank': 0,\n",
            "  'max_grad_norm': 1.0,\n",
            "  'max_length': 156,\n",
            "  'model_class_name': 'bert-base',\n",
            "  'node_rank': 0,\n",
            "  'num_nodes': 1,\n",
            "  'per_gpu_eval_batch_size': 16,\n",
            "  'per_gpu_train_batch_size': 4,\n",
            "  'resume_path': None,\n",
            "  'sampler_seed': -1,\n",
            "  'save_prediction:': True,\n",
            "  'seed': 1,\n",
            "  'single_gpu': True,\n",
            "  'total_step': -1,\n",
            "  'train_data': 'anli_r1_train:none',\n",
            "  'train_weights': '10',\n",
            "  'warmup_steps': -1,\n",
            "  'weight_decay': 0.0,\n",
            "  'world_size': 1}\n",
            "Arguments None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "091b340f6e6a4ce2bbeca43704fa944d",
            "61852593b7794ea997db5f00381bcb29",
            "1628ee35762549499ebe8fca7b547f00",
            "24c5f4eda2814d57a224573f7cbe990c",
            "bf2e6b1664fc4dbbb70bfa7ffd47fbc2",
            "cf0d0d20f1ee449592d22f08d52f23cc",
            "8b2c485ed19e4d9b8d8b457cb3b9e29a",
            "679afbb04ebe4178b7ce54cb23d28ff4",
            "29c5d1907a8245c3990d8a567f29c2ae",
            "be0935ce53364f55a12580076601ff1b",
            "786879b25be943d2bda169b3bc5ecbc9",
            "effb8b65364745f79a8a3816a154ab1e",
            "a7360efb58ef4603bc487d5e5ad2342c",
            "27d9e9c9f4414d2eb4aa4babae3282d0",
            "3308b91ae33542c3b7b1b5598d4d699f",
            "95c63806fc7c4750bbc92f1653a333ec"
          ]
        },
        "id": "HptyjkfXT37f",
        "outputId": "e4d99134-afc2-47b3-e3a2-4c7dab1ad133"
      },
      "source": [
        "for epoch in tqdm(range(num_epoch), desc=\"Epoch\", disable=global_rank not in [-1, 0]):\n",
        "  # Let's build up training dataset for this epoch\n",
        "  training_list = []\n",
        "  for i in range(len(train_data_list)):\n",
        "      print(\"Build Training Data ...\")\n",
        "      train_d_list = train_data_list[i]\n",
        "      train_d_name = train_data_name[i]\n",
        "      train_d_weight = train_data_weights[i]\n",
        "      cur_train_list = sample_data_list(train_d_list, train_d_weight)  # change later  # we can apply different sample strategy here.\n",
        "      print(f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"\n",
        "            f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\")\n",
        "      training_list.extend(cur_train_list)\n",
        "\n",
        "  random.shuffle(training_list)\n",
        "  train_dataset = NLIDataset(training_list, data_transformer)\n",
        "\n",
        "  train_sampler = SequentialSampler(train_dataset)\n",
        "  if not cpu and not single_gpu:\n",
        "      print(\"Use distributed sampler.\")\n",
        "      train_sampler = DistributedSampler(train_dataset, world_size, global_rank,\n",
        "                                          shuffle=True)\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                batch_size=batch_size_per_gpu_train,\n",
        "                                shuffle=False,  #\n",
        "                                num_workers=0,\n",
        "                                pin_memory=True,\n",
        "                                sampler=train_sampler,\n",
        "                                collate_fn=BaseBatchBuilder(batching_schema))  #\n",
        "\n",
        "  if not cpu and not single_gpu:\n",
        "      if sampler_seed == -1:\n",
        "          train_sampler.set_epoch(epoch)  # setup the epoch to ensure random sampling at each epoch\n",
        "      else:\n",
        "          train_sampler.set_epoch(epoch + sampler_seed)\n",
        "\n",
        "  for forward_step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\",\n",
        "                                            disable=global_rank not in [-1, 0]), 0):\n",
        "      model.train()\n",
        "\n",
        "      batch = move_to_device(batch, local_rank)\n",
        "      # print(batch['input_ids'], batch['y'])\n",
        "      if model_class_name in [\"distilbert\", \"bart-large\"]:\n",
        "          outputs = model(batch['input_ids'],\n",
        "                          attention_mask=batch['attention_mask'],\n",
        "                          labels=batch['y'])\n",
        "      else:\n",
        "          outputs = model(batch['input_ids'],\n",
        "                          attention_mask=batch['attention_mask'],\n",
        "                          token_type_ids=batch['token_type_ids'],\n",
        "                          labels=batch['y'])\n",
        "      loss, logits = outputs[:2]\n",
        "      # print(debug_node_info(args), loss, logits, batch['uid'])\n",
        "      # print(debug_node_info(args), loss, batch['uid'])\n",
        "\n",
        "      # Accumulated loss\n",
        "      if gradient_accumulation_steps > 1:\n",
        "          loss = loss / gradient_accumulation_steps\n",
        "\n",
        "      # if this forward step need model updates\n",
        "      # handle fp16\n",
        "      if fp16:\n",
        "          with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "              scaled_loss.backward()\n",
        "      else:\n",
        "          loss.backward()\n",
        "\n",
        "          # Gradient clip: if max_grad_norm < 0\n",
        "      if (forward_step + 1) % gradient_accumulation_steps == 0:\n",
        "          if max_grad_norm > 0:\n",
        "              if fp16:\n",
        "                  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
        "              else:\n",
        "                  torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "          optimizer.step()\n",
        "          scheduler.step()  # Update learning rate schedule\n",
        "          model.zero_grad()\n",
        "\n",
        "          global_step += 1\n",
        "\n",
        "          if global_rank in [-1, 0] and eval_frequency > 0 and global_step % eval_frequency == 0:\n",
        "              r_dict = dict()\n",
        "              # Eval loop:\n",
        "              for i in range(len(eval_data_name)):\n",
        "                  cur_eval_data_name = eval_data_name[i]\n",
        "                  cur_eval_data_list = eval_data_list[i]\n",
        "                  cur_eval_dataloader = eval_data_loaders[i]\n",
        "                  # cur_eval_raw_data_list = eval_raw_data_list[i]\n",
        "\n",
        "                  evaluation_dataset(args_dict, cur_eval_dataloader, cur_eval_data_list, model, r_dict,\n",
        "                                      eval_name=cur_eval_data_name)\n",
        "\n",
        "              # saving checkpoints\n",
        "              current_checkpoint_filename = \\\n",
        "                  f'e({epoch})|i({global_step})'\n",
        "\n",
        "              for i in range(len(eval_data_name)):\n",
        "                  cur_eval_data_name = eval_data_name[i]\n",
        "                  current_checkpoint_filename += \\\n",
        "                      f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'\n",
        "\n",
        "              if not debug_mode:\n",
        "                  # save model:\n",
        "                  model_output_dir = checkpoints_path / current_checkpoint_filename\n",
        "                  if not model_output_dir.exists():\n",
        "                      model_output_dir.mkdir()\n",
        "                  model_to_save = (\n",
        "                      model.module if hasattr(model, \"module\") else model\n",
        "                  )  # Take care of distributed/parallel training\n",
        "\n",
        "                  torch.save(model_to_save.state_dict(), str(model_output_dir / \"model.pt\"))\n",
        "                  torch.save(optimizer.state_dict(), str(model_output_dir / \"optimizer.pt\"))\n",
        "                  torch.save(scheduler.state_dict(), str(model_output_dir / \"scheduler.pt\"))\n",
        "\n",
        "              # save prediction:\n",
        "              if not debug_mode and save_prediction:\n",
        "                  cur_results_path = prediction_path / current_checkpoint_filename\n",
        "                  if not cur_results_path.exists():\n",
        "                      cur_results_path.mkdir(parents=True)\n",
        "                  for key, item in r_dict.items():\n",
        "                      common.save_jsonl(item['predictions'], cur_results_path / f\"{key}.jsonl\")\n",
        "\n",
        "                  # avoid saving too many things\n",
        "                  for key, item in r_dict.items():\n",
        "                      del r_dict[key]['predictions']\n",
        "                  common.save_json(r_dict, cur_results_path / \"results_dict.json\", indent=2)\n",
        "\n",
        "          if total_step > 0 and global_step == t_total:\n",
        "              # if we set total step and global step s t_total.\n",
        "              is_finished = True\n",
        "              break\n",
        "\n",
        "  # End of epoch evaluation.\n",
        "  if global_rank in [-1, 0] and total_step <= 0:\n",
        "      r_dict = dict()\n",
        "      # Eval loop:\n",
        "      for i in range(len(eval_data_name)):\n",
        "          cur_eval_data_name = eval_data_name[i]\n",
        "          cur_eval_data_list = eval_data_list[i]\n",
        "          cur_eval_dataloader = eval_data_loaders[i]\n",
        "          # cur_eval_raw_data_list = eval_raw_data_list[i]\n",
        "\n",
        "          evaluation_dataset(args, cur_eval_dataloader, cur_eval_data_list, model, r_dict,\n",
        "                              eval_name=cur_eval_data_name)\n",
        "\n",
        "      # saving checkpoints\n",
        "      current_checkpoint_filename = \\\n",
        "          f'e({epoch})|i({global_step})'\n",
        "\n",
        "      for i in range(len(eval_data_name)):\n",
        "          cur_eval_data_name = eval_data_name[i]\n",
        "          current_checkpoint_filename += \\\n",
        "              f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'\n",
        "\n",
        "      if not debug_mode:\n",
        "          # save model:\n",
        "          model_output_dir = checkpoints_path / current_checkpoint_filename\n",
        "          if not model_output_dir.exists():\n",
        "              model_output_dir.mkdir()\n",
        "          model_to_save = (\n",
        "              model.module if hasattr(model, \"module\") else model\n",
        "          )  # Take care of distributed/parallel training\n",
        "\n",
        "          torch.save(model_to_save.state_dict(), str(model_output_dir / \"model.pt\"))\n",
        "          torch.save(optimizer.state_dict(), str(model_output_dir / \"optimizer.pt\"))\n",
        "          torch.save(scheduler.state_dict(), str(model_output_dir / \"scheduler.pt\"))\n",
        "\n",
        "      # save prediction:\n",
        "      if not debug_mode and save_prediction:\n",
        "          cur_results_path = prediction_path / current_checkpoint_filename\n",
        "          if not cur_results_path.exists():\n",
        "              cur_results_path.mkdir(parents=True)\n",
        "          for key, item in r_dict.items():\n",
        "              common.save_jsonl(item['predictions'], cur_results_path / f\"{key}.jsonl\")\n",
        "\n",
        "          # avoid saving too many things\n",
        "          for key, item in r_dict.items():\n",
        "              del r_dict[key]['predictions']\n",
        "          common.save_json(r_dict, cur_results_path / \"results_dict.json\", indent=2)\n",
        "\n",
        "  if is_finished:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091b340f6e6a4ce2bbeca43704fa944d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='iâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Build Training Data ...\n",
            "Data Name:anli_r1_train; Weight: 10.0; Original Size: 16946; Sampled Size: 169460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29c5d1907a8245c3990d8a567f29c2ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=42365.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}